<!DOCTYPE html>
<html lang="en">
<head>
<title>ICDAR 2023 Competition on Language Guided Document Editing (DocEdit)</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<link href="layout/styles/layout.css" rel="stylesheet" type="text/css" media="all">
</head>
<body id="top">
<div class="wrapper row1">
  <header id="header" class="hoc clear"> 
    <!-- ################################################################################################ -->
    <div id="logo" class="fl_left">
      <h1><a href="index.html">DocEdit@ICDAR 2023</a></h1>
    </div>
    <nav id="mainav" class="fl_right">
      <ul class="clear">
        <li class="active"><a href="index.html">Home</a></li>
      </ul>
    </nav>
    <!-- ################################################################################################ -->
  </header>
</div>
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<div class="wrapper bgded overlay" style="background-image:url('images/intro3-edited.jpg');">
  <div id="pageintro" class="hoc clear"> 
    <!-- ################################################################################################ -->
      <h3 class="heading">ICDAR 2023 Competition on Language Guided Document Editing (DocEdit)</h3>
      <h4>August 21-26, 2023</h4>
      <h4>Location: San José, California, USA</h4>
    <!-- ################################################################################################ -->
  </div>
</div>
<!-- ################################################################################################ -->
<div class="wrapper row3"  style="background-color: whitesmoke;">
  <main class="hoc container clear">
    <!-- main body -->
    <!-- ################################################################################################ -->
    <section>
      <div>
        <h6 class="heading">News</h6>
        <ul>
          <li><strong>August 21-26, 2023</strong>: DocEdit Competition at ICDAR 2023</li>
          <li><strong>Mar 15, 2023</strong>: DocEdit is open for submission</li>
          <li><strong>Mar 10, 2023</strong>: DocEdit website is launched</li>
        </ul>
      </div>
    </section>
  </main>
</div>

<div class="wrapper row3"  style="background-color: lightgray;">
  <main class="hoc container clear">
    <section>
      <div>
        <h6 class="heading">About the competition</h6>
        <p>
          Billions of digital documents and PDF's are created, edited or shared each year ubiquitously, with a majority of them being designed by amateur users. Professional document editing requires a certain level of expertise to perform complex edit operations. To make editing tools accessible to increasingly novice users, there is a need for intelligent document assistant systems that can make or suggest edits based on a user’s natural language request. Such a system should be able to understand the user’s ambiguous requests and contextualize them to the visual cues and textual content found in a document image to edit the text and its structured layout. Unparalleled advances in deep learning, language/foundation models, and generative AI has made it possible to utilize multimodal cues for automated document manipulation. However, there are several unresolved challenges towards realizing this goal: (1) extracting intent, sequence of actions, and visual attributes of the document, (2) document hierarchical parsing for layout understanding, (3) understanding direct and indirect references to document objects, (4) inferring local and global relations between embedded text and visual objects through multimodal (text+visual+layout) signals, and (5) grounding the requested edits to localized objects and scene text. To address all these major challenges, it is critical that we develop more reliable and accurate AI techniques for automated document editing  that can handle the inherent complexity of the task. To this end, we propose the first "ICDAR 2023 Competition on Language-Guided Document Editing (DocEdit)". The main goal of this competition is to bring together researchers from various domains such as natural language processing, computer vision, machine learning, human computer interaction, data mining, graphics, and multimedia to explore artificial intelligence solutions for language-guided document editing.
 
        </p>
        <p>
          This competition will be of interest to researchers working in natural language processing, computer vision, multimodal deep learning, document intelligence, signal processing, artificial intelligence, information extraction and retrieval, and data mining and in particular to those who are interested in the applications of AI in document understanding. ICDAR 2023 will be sought after destination for researchers from document analysis and representation, computer vision and language communities. With the rise of Transformer and Stable Diffusion based techniques in AI, the competition will be of very high interest to researchers in generative AI and vision+language modeling. The competition aims to attract young researchers from universities, early-career AI practitioners as well as industry veterans in the document intelligence space.
        </p>

      </div>
    </section>
  </main>
</div>
  
<div class="wrapper row3"  style="background-color: lightgray;">
  <main class="hoc container clear">
    <section>
      <div>
        <h6 class="heading">Competition Outline</h6>
        <p>
          <h5 class="heading">Dataset</h6>
            DocEdit dataset provides pairs of document images and user edit requests along with the ground truth edit command. Each edit request is mapped to an executable command that can be simulated in a real-world document editing software. We present 17K scanned PDF documents comprising edits performed on publicly available PDF documents. The dataset has a diverse mix of edit operations (add, delete, modify, split, merge, replace, move, copy) and reference types (direct, object referring, text referring) from the users.
        </p>
       <h5 class="heading">Evaluation Rules </h6>
        <p>
          On March, 2023, we will release the test datasets, and by March 15th, 2023 competition participants are expected submit the following:
          <br> 1. Predictions on each test dataset
          <br> 2. A short but complete system description
         <br> The organizers will tabulate the results for each task and present it at ICDAR 2023 in San Jose, California, USA . Note that you do not need to attend ICDAR 2023 to participate in this competition.
        </p>
    <h5 class="heading">Tasks</h5>
        <p>
          <ul>
              <li>Sub-task 1: Edit Command Generation: Input - document image to be edited and the user text request output; Output - ACTION (< Component >, < Attribute >, < Initial_State >, < Final_State >) </li>
                <br> Action describes the executable function belonging to the following taxonomy - Add, Delete, Copy, Move, Replace, Split, Merge, Modify. It is followed by arguments corresponding to the document components to be edited, attributes to be modified, initial state of the attributes, and the final state of the attributes expected in the edited version.
              <li>Sub-task 2: Bounding Box RoI prediction: Input - document image to be edited and the user text request output; Output - [x, y, h, w]</li>
                <br> (x, y) refers to the top-left coordinate and (h, w) refer to the height and width of the bounding box
          </ul>
          <br>Each subtask is evaluated in isolation, meaning that systems have access to the ideal output (i.e. Ground Truth) of previous subtasks. 
        </p>  
    <h5 class="heading">Evaluation Metrics</h5>
        <p>
          <ul>
              <li>Sub-task 1: Edit Command Generation: Exact match accuracy (EM %) and ROUGE score.</li>
              <li>Sub-task 2: Bounding Box RoI prediction: Top-1 accuracy (%). (Jaccard overlap between the predicted region and the ground-truth > 0.5</li>
          </ul>
          <br>Each subtask is evaluated in isolation.
        </p>

    <h5 class="heading">Competition Rules</h5>
      <p>
       <ul>
  
          <li>The competition is open for participants from both industry and academia. The competition is open for both students and professionals who want to make a contribution to the field of chart recognition. Below we describe the rules for our competition.</li>
          <li>lEach participant team can include up to a maximum of 10 people from one or more affiliations. For the sake of fairness to smaller research groups, we will not allow bigger research groups to participate as a single team.</li>
          <li>One person can only participate on one team. Mentors included. No exceptions.</li>
          <li>Winners teams will have certificates listing the names of their members in the exact format and order as they were registered.</li>
          <li>Participants can register to participate on sub-tasks. </li>
          <li>For each sub-task, we will rank all participant teams seperately.</li>
          <li>The total scores for both tasks will be added up for each participant team to obtain a single final score used to pick the overall winning team.</li>
          <li>Participants will be allowed to use their own datasets for training. However, they must notify the organizers about this.</li>
          <li>Submissions will be done via e-mail. Participants will be required to submit the results of their systems in the expected JSON format. We might not consider submissions which do not conform to the correct file format.</li>
          <li> CODE submission will be required.</li>
          <li>Participants must provide a description of the methods used to produce the results submitted. In the final competition paper, we will summarize these descriptions when we describe the submitted systems. If external datasets were used, participants will also have to provide a full description of these. We reserve the right to disqualify submissions which do not provide a sufficiently detailed description of their system.</li>
          <li>To be fair to all participants, any deadline extensions given will apply to all participants, not just to individual research groups who might request them. </li>
       </ul>
    <h5 class="heading">Data Download</h5>
        <table>
          <tr>
            <th>Command Generation</th>
            <th><a href="index.html">Training</a></th>
            <th><a href="index.html">Eval</a></th>
          </tr>
          <tr>
            <td>RoI Bounding Box</td>
            <th><a href="index.html">Training</a></th>
            <th><a href="index.html">Eval</a></th>
          </tr>
        </table>
          </div>
    </section>
  </main>
</div>
<div class="wrapper row3"  style="background-color: whitesmoke;">
  <main class="hoc container clear">
    <section>
      <div>
        <h6 id="important-date" class="heading">Important dates</h6>
        <ul>
          <li>DocEdit is accepted at ICDAR 2023: January 2023</li>
          <li>DocEdit website is launched: March 10, 2023</li>
          <li>DocEdit is open for submission: March 15, 2023</li>
          <li>Submission deadline: April 10, 2023</li>
          <li>Winners Announced: April 12, 2023</li>
          <li><strong>DocEdit competition at ICDAR 2023:</strong> Aug 21-26, 2023</li>
        </ul>
        <p>All deadlines are end of day, anywhere on earth (UTC-12).</p>
      </div>
    </section>
  </main>
</div>


<div class="wrapper row3"  style="background-color: lightgray;">
  <main class="hoc container clear">
    <section>
      <div>
        <h6 id="organizer" class="heading">Organizers</h6>
        <ul>
          <li><a href="http://www.cs.umd.edu/~puneetm/">Puneet Mathur</a>, University of Maryland College Park, USA</li>
          <li><a href="https://research.adobe.com/person/rajiv-jain/">Rajiv Jain</a>, Adobe Research, USA</li>
          <li><a href="https://research.adobe.com/person/jiuxiang-gu/">Jiuxiang Gu</a>, Adobe Research, USA</li>
          <li><a href="https://research.adobe.com/person/franck-dernoncourt/">Franck Dernoncourt</a>, Adobe Research, USA</li>
          <li><a href="https://research.adobe.com/person/vlad-morariu/">Vlad Morariu</a>, Adobe Research, USA</li>
          <li><a href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>, University of Maryland College Park, USA</li>
        </ul>
      </div>
    </section>
  </main>
</div>


<!-- <div class="wrapper row3"  style="background-color: lightgray;">
  <main class="hoc container clear">
    <section>
      <div>
        <h6 id="organizer" class="heading">Program Committee</h6>
        <ul>
          
          <li>Miguel Villarreal-Vasquez, JP Morgan AI</li>
          <li>Yao Xuan, Meta</li>       
          <li>Mihir Goyal, Sharechat</li>
          <li>Sreyan Ghosh, University of Maryland College Park</li>
          <li>Shishira Maiya, University of Maryland College Park</li>
          <li>Yow-Ting Shiue, University of Maryland College Park</li>
          <li>Apoorv Singh, Motional</li>
          <li>Vineet Malhotra, MIDAS IIIT Delhi</li>
          
        </ul>
      </div>
    </section>
  </main>
</div> -->
  
<div class="wrapper row3"  style="background-color: midnightblue;">
  <main class="hoc container clear">
    <div class="clear">
      <p style="color: antiquewhite;">Contact us: docedit.icdar2023@gmail.com or puneetm@umd.edu</p>
    </div>
  </main>
</div>


<a id="backtotop" href="#top"><i class="fa fa-chevron-up"></i></a>
<!-- JAVASCRIPTS -->
<script src="layout/scripts/jquery.min.js"></script>
<script src="layout/scripts/jquery.backtotop.js"></script>
<script src="layout/scripts/jquery.mobilemenu.js"></script>
</body>
</html>
